# Exercise 3: Add AI Chat to your application

> [!Note]
> Goal: Add AI chat functionality and a LLM, then connect the chat interface in your app to Azure OpenAI and AI Services.

## Module overview

In [Exercise-2](/Lab-Instructions/2.Exercise-2.md), you added Azure AI Search and made sure your app can connect to the Search service end point. You have also created an index and uploaded some sample data to the Search service.

In this module, you are going to use `azd add` to add two AI models:
1. Azure OpenAI (GPT-4o) for primary chat functionality
2. Azure AI Services Foundry model (Phi-4) to compare different model responses

This will let you learn how your chat result is affected when you add grounding data and when you switch between models.

> [!Note]
> Time to complete: 25 minutes

## Instructions

1. Make sure you are still in the **/src** folder.
1. First, add the GPT-4o model:
    * Run `azd add`
    * Select "**AI**"
    * Select "**Azure OpenAI model**"
    * Select "**Chat (GPT)**"
    * Select "**gpt-4o 2024-11-20**"
    * For this lab exercise, when prompted to provide a name, hit enter to select the default value which is "gpt-4o" 
    * Make sure your service uses the newly added resource by typing \<space> to select.
    ![Connect service to gpt-4o](/Lab-Instructions/Images/3.ConnectServicetoAOAI.png)
    * Type "**Y**" or simply hit \<enter> to accept changes to azure.yaml.
    * Select "**No**" to **SKIP** provisioning the changes. We will add another model before provisioning.

1. Next, add the Phi-4 model from AI Foundry:
    * Run `azd add` again
    * Select "**AI**"
    * Select "**Azure AI services model**"
    * Select "**Phi-4**" by typing or using arrow keys (**app.py** has hardcoded the model to be Phi-4 for Foundry model)
    * Select "**AIServices**" for deployment kind.
    * Select "**7**" for version.
    * Make sure you type \<SPACE> to connect **src** to the new **ai-project** resource.
    * Type "**Y**" or simply hit \<enter> to accept changes to azure.yaml.
    * Select "**Yes**" to provision the changes. When prompted for the **phi47Location** parameter, use **East US 2**.

1. Provision can take 5-8 minutes.

1. While waiting, use GitHub Copilot for Azure to learn more about AI models. Try these sample prompts:
    
    * "@azure can you provide a comparison of AI models that are appropriate for Azure OpenAI and a chat app like the one in this lab?"
    * "@azure how is Phi-4 model different from gpt-4o?"
    * "@azure why would I choose one over the other?"
    
    The OpenAI Chat GPT-4o model we're using is a powerful language model available through the Azure OpenAI Service. This model is designed for advanced conversational AI tasks.
    
    GPT-4o is integrated with Azure's capabilities, providing scalability, security, and enterprise features. There are different variants like GPT-4o Mini, which is a lighter version, and GPT-4 Turbo with Vision, which includes vision capabilities, that you can use for your use case.

    The Phi-4 model ...

## Running the app and testing the models

We will skip running the app locally but similar to the previous exercise, if you want to do that, run `azd show gpt-4o` and set the environment variable. Reminder: Line 33 in app.py.

![app.py](/Lab-Instructions/Images/3.appcode.png)

If you didn't change any of the app code, you **do not need** to run `azd deploy`. 

**Hint**: You can always run `azd show` to get the endpoint of your app running on Azure if you don't already have it open.

### Testing basic chat functionality

1. Go to your app endpoint.
2. Under AI Hotel Recommendations, click the "Get Recommendations" button. Chat will say "I don't know".
3. Switch the "Use grounding with AI Search" toggle to on and click "Get Recommendations" button and you will see:

    ![Chat with grounding data](/Lab-Instructions/Images/3.chat-grounding.png)

4. You can ask Copilot to explain how this works:
   Sample prompt: "@azure can you explain how does grounding data to search AI work for this app?"

### Comparing models

The app has a toggle at the top right corner that allows you to switch between the GPT-4o and Phi-4 Foundry model.

![Foundry toggle](/Lab-Instructions/Images/3.foundrytoggle.png)

Since we added both models, toggle it on to compare and see the difference!
![Comparison of models](/Lab-Instructions/Images/3.Comparision.png)

## The Code

At this point, your azure.yaml should look like this:

```yaml
# yaml-language-server: $schema=https://raw.githubusercontent.com/Azure/azure-dev/main/schemas/alpha/azure.yaml.json
name: src
metadata:
  template: azd-init@xxxx
services:
  src:
    project: .
    host: containerapp
    language: python
    docker:
      path: Dockerfile
resources:
  src:
    type: host.containerapp
    port: 5000
    uses:
      - search
      - gpt-4o
      - ai-project
  search:
    type: ai.search
  gpt-4o:
    type: ai.openai.model
    model:
      name: gpt-4o
      version: "2024-11-20"
  ai-project:
    type: ai.project
    models:
      - name: Phi-4
        version: "7"
        format: Microsoft
        sku:
          name: GlobalStandard
          usageName: AIServices.GlobalStandard.MaaS
          capacity: 1
```

## Next
Now that you've added AI Chat to your application, head over to [Exercise 4](/Lab-Instructions/4.Exercise-4.md) to configure your applications so it's running optimally on Azure.
